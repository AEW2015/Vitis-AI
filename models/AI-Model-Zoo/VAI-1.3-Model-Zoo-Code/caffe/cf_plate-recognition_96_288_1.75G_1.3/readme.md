### Contents
1. [Installation](#installation)
2. [Preparation](#preparation)
3. [Train/Eval](#traineval)
4. [Performance](#performance)
5. [Model_info](#model_info)  

### Installation
1. Get xilinx caffe-xilinx code. 
  ```shell
  cd caffe-xilinx
  ```

2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.
  ```shell
  # Modify Makefile.config according to your Caffe installation.
  cp Makefile.config.example Makefile.config
  make -j8
  # python2
  make pycaffe
  ```
3. Install lmdb for lmdb generation.
  ```
  pip install lmdb==1.0.0  ## lmdb is needed in code/gen_data/createLableLMDB.py
  ```

  Note: If you are in the released Docker env, there is no need to build Caffe.

### Preparation

1. Prepare dataset.
  ```  
  a. dataset includes images, groundtruth file and fake label file(generated by code/gen_data/createLableLMDB.py with groundtruth file).
    1) groundtruth file format(imagename license_number license_color) as following:
       3406.jpg 皖G92645 blue
       3842.jpg 闽DP708E blue
    2) fake label file is used to generate data lmdb and format as following:
       3406.jpg 0
       3842.jpg 0
  b. dataset structure:
     data/
         + train_images/
         + train_gt.txt
         + val_images/
         + val_gt.txt  
  ```
2. Preprocess dataset.
  
  a.generate label lmdb and fake label file by using "code/gen_data/createLableLMDB.py" with groundtruth file.
  ```
  # for user using vitis-ai docker, uncomment to setup docker environments:
  # bash setup_docker_locales.sh
  # export LANG="en_US.UTF-8"
  mkdir data/plate_recognition_val
  cd code/gen_data/
  python createLableLMDB.py --train-gt-file "path/to/train/gt"(like ../../data/train_gt.txt) --val-gt-file "path/to/val/gt"(like ../../data/val_gt.txt)  
  ```  
  then would generate fake files(like plate_train_fake.txt, plate_val_fake.txt) and label lmdbs(like train_plate_recognition_label_lmdb, val_plate_recognition_label_lmdb) in `plate_recognition_val`. 

  b.generate image lmdb by using fake label files 
  ```
  caffe-xilinx/build/tools/convert_imageset "path/to/train_images" "path/to/plate_train_fake.txt" train_plate_recognition_lmdb --resize_height 96 --resize_width 288.

  caffe-xilinx/build/tools/convert_imageset "path/to/val_images" "path/to/plate_val_fake.txt" val_plate_recognition_lmdb --resize_height 96 --resize_width 288.
  
  # for vitis-ai docker env, use:
  # /opt/vitis_ai/conda/envs/vitis-ai-caffe/bin/convert_imageset "path/to/train_images" "path/to/plate_train_fake.txt" train_plate_recognition_lmdb --resize_height 96 --resize_width 288.

  # /opt/vitis_ai/conda/envs/vitis-ai-caffe/bin/convert_imageset "path/to/val_images" "path/to/plate_val_fake.txt" val_plate_recognition_lmdb --resize_height 96 --resize_width 288.
 
  ```
  finally move `train_plate_recognition_lmdb` and `val_plate_recognition_lmdb` to `data`.

  
### Train/Eval
1. Train your model and evaluate the model on the fly.
  ```shell
  # cd train of this model.
  cd code/train
  # modify configure if you need, includs caffe root, solver configure.
  bash train.sh 
  ```

2. Evaluate caffemodel.
  ```shell
  # cd test of this model
  cd code/test
  # modify configure if you need, includes caffe root, model path, weight path... 
  python test.py
  ```

3. Evaluate quantized model.
  ```shell
  # cd test of this model
  cd code/test
  # modify configure if you need, includes caffe root, model path, weight path... 
  bash quantized_test.sh
  ```

### Performance
 
Evaluate on private dataset with 6777 images.  

|Acc |Float model performance on dataset| 
|----|----|
|plate number(%)|99.51|
|plate color(%)|100|

|Acc |Quantized(int8) model performance on dataset| 
|----|----|
|plate number(%)|99.51|
|plate color(%)|100|

### Model_info

1.data preprocess
```
1. data channel order: BGR(0~255)                  
2. resize: 96*288(h*w).
3. mean_value: 104, 117, 123
4. scale: 1.0
```
2.For quantization with calibration mode:
  ```
  Modify datalayer of test.prototxt for model quantization:
  a. Replace the "Input" data layer of test.prototxt with the "ImageData" data layer.
  b. Modify the "ImageData" layer parameters according to the data preprocess information.
  c. Provide a "quant.txt" file, including image path and label information with fake value(like 1).
  d. Give examples of data layer and "quant.txt":

  # data layer example
    layer {
    name: "data"
    type: "ImageData"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      mirror: false
      mean_value: 104
      mean_value: 117
      mean_value: 123
     }

    image_data_param {
      source: "quant.txt"
      new_width: 288 
      new_height: 96
      batch_size: 16
    }
  }
  # quant.txt: image path label
    images/000001.jpg 1
    images/000002.jpg 1
    images/000003.jpg 1

  ```
3.For quantization with finetuning mode: 
  ```
  use trainval.prototxt for model quantization.
  ```
